{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import os\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.feature import Imputer, StandardScaler, VectorAssembler, PCA\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.sql import functions as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('bankruptcy_prediction').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the five years of data\n",
    "num_partitions = 20\n",
    "year1 = spark.read.csv(\"data/csv_result-1year.csv\", header=True, inferSchema=True)\n",
    "year2 = spark.read.csv(\"data/csv_result-2year.csv\", header=True, inferSchema=True)\n",
    "year3 = spark.read.csv(\"data/csv_result-3year.csv\", header=True, inferSchema=True)\n",
    "year4 = spark.read.csv(\"data/csv_result-4year.csv\", header=True, inferSchema=True)\n",
    "year5 = spark.read.csv(\"data/csv_result-5year.csv\", header=True, inferSchema=True)\n",
    "df_raw = year1.union(year2).union(year3).union(year4).union(year5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the data for 0s and ?\n",
    "\n",
    "float_cols = ['Attr' + str(i) for i in range(1, 65)]\n",
    "\n",
    "for col_name in float_cols:\n",
    "    df_raw = df_raw.withColumn(col_name, when((col(col_name) == '0') | (col(col_name) == '?'), None).otherwise(col(col_name)))\n",
    "    \n",
    "# cast columns to float\n",
    "for col_name in float_cols:\n",
    "    df_raw = df_raw.withColumn(col_name, df_raw[col_name].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_raw.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputing missing values\n",
    "imputer = Imputer(inputCols=df_raw.columns, outputCols=[\"{}_imputed\".format(c) for c in df_raw.columns])\n",
    "imputed_df = imputer.fit(df_raw).transform(df_raw)\n",
    "imputed_df = imputed_df.drop(*imputed_df.columns[:66])\n",
    "imputed_df = imputed_df.withColumnRenamed('class_imputed','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split train/test\n",
    "train_df, test_df = imputed_df.randomSplit([.75, .25], seed=42)\n",
    "\n",
    "train_df = train_df.repartition(num_partitions).cache()\n",
    "test_df = test_df.repartition(num_partitions).cache()\n",
    "\n",
    "X_train = train_df.drop('label', 'id_imputed')\n",
    "Y_train = train_df.select('label')\n",
    "X_test = test_df.drop('label', 'id_imputed')\n",
    "Y_test = test_df.select('label')\n",
    "\n",
    "# Assemble all feature columns into a single vector column\n",
    "assembler = VectorAssembler(inputCols=X_train.columns, outputCol='features', handleInvalid=\"skip\")\n",
    "X_train = assembler.transform(X_train)\n",
    "X_test = assembler.transform(X_test)\n",
    "\n",
    "# Standardize the feature vector column\n",
    "scaler = StandardScaler(inputCol='features', outputCol='scaled_features')\n",
    "scaler_model = scaler.fit(X_train)\n",
    "X_train = scaler_model.transform(X_train).drop('features')\n",
    "X_test = scaler_model.transform(X_test).drop('features')\n",
    "\n",
    "# Adding index to join Y_train and Y_test with X_train and X_test\n",
    "X_train = X_train.withColumn('index', F.monotonically_increasing_id())\n",
    "Y_train = Y_train.withColumn('index', F.monotonically_increasing_id())\n",
    "X_test = X_test.withColumn('index', F.monotonically_increasing_id())\n",
    "Y_test = Y_test.withColumn('index', F.monotonically_increasing_id())\n",
    "\n",
    "X_train = X_train.join(Y_train, on='index', how='inner').drop('index')\n",
    "X_test = X_test.join(Y_test, on='index', how='inner').drop('index')\n",
    "\n",
    "X_train = X_train.repartition(num_partitions).cache()\n",
    "X_test = X_test.repartition(num_partitions).cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE implementation\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from pyspark.ml.linalg import Vectors, DenseVector\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "def dense_vector_to_list(vector: DenseVector):\n",
    "    return vector.toArray().tolist()\n",
    "\n",
    "dense_vector_to_list_udf = udf(dense_vector_to_list, ArrayType(DoubleType()))\n",
    "\n",
    "X_train_pd = X_train.select(dense_vector_to_list_udf('scaled_features').alias('scaled_features_list'), 'label').toPandas()\n",
    "X_train_list = X_train_pd['scaled_features_list'].tolist()\n",
    "y_train_list = X_train_pd['label'].tolist()\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_list, y_train_list)\n",
    "\n",
    "X_train_smote_df = spark.createDataFrame([(int(y), Vectors.dense(x)) for x, y in zip(X_train_smote, y_train_smote)], schema=['label', 'scaled_features'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "start_time = time.time()\n",
    "pca = PCA(k=10, inputCol='scaled_features', outputCol='reduced_features')\n",
    "pca_model = pca.fit(X_train_smote_df)\n",
    "\n",
    "pca_train = pca_model.transform(X_train_smote_df).select('label', 'reduced_features').withColumnRenamed('reduced_features', 'features')\n",
    "pca_test = pca_model.transform(X_test).select('label', 'reduced_features').withColumnRenamed('reduced_features', 'features')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f'PCA Train time: {end_time - start_time:.3f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label', maxIter=100)\n",
    "\n",
    "\n",
    "paramGrid = (ParamGridBuilder().addGrid(lr.regParam, [0.1, 0.01, 0.001]).addGrid(lr.elasticNetParam, [0.0]).build())\n",
    "\n",
    "crossval = CrossValidator(estimator=lr,estimatorParamMaps=paramGrid,evaluator=BinaryClassificationEvaluator(),numFolds=4)\n",
    "# Train the Logistic Regression model                    \n",
    "lr_model = crossval.fit(pca_train)                          \n",
    "#Make predictions on the test set\n",
    "predictions = lr_model.transform(pca_test)\n",
    "\n",
    "# Evaluate the model\n",
    "binary_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "roc_auc = binary_evaluator.evaluate(predictions, {binary_evaluator.metricName: \"areaUnderROC\"})\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"rmse\")\n",
    "accuracy = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
    "f1_score = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"f1\"})\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f'Logistic Regression evaluation time: {end_time - start_time:.3f} seconds')                         \n",
    "print('Logistic Regression')\n",
    "print(\"ROC-AUC: {:.5f}\".format(roc_auc))\n",
    "print(\"Accuracy: {:.6f}\".format(accuracy))\n",
    "print(\"F1 Score: {:.5f}\".format(f1_score))\n",
    "print(\"RMSE: {}\".format(rmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the elements of the confusion matrix\n",
    "TN = predictions.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = predictions.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = predictions.filter('prediction = 0 AND label <> prediction').count()\n",
    "FP = predictions.filter('prediction = 1 AND label <> prediction').count()\n",
    "# show confusion matrix\n",
    "predictions.groupBy('label', 'prediction').count().show()\n",
    "# calculate metrics by the confusion matrix\n",
    "accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "F =  2 * (precision*recall) / (precision + recall)\n",
    "# calculate auc\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "auc = evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderROC'})\n",
    "print('n precision: %0.3f' % precision)\n",
    "print('n recall: %0.3f' % recall)\n",
    "print('n accuracy: %0.3f' % accuracy)\n",
    "print('n F1 score: %0.3f' % F)\n",
    "print('AUC: %0.3f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "start_time = time.time()\n",
    "# Train the RandomForestClassifier model\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol='label', numTrees=100)\n",
    "\n",
    "binary_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "# Create a parameter grid for hyperparameter tuning\n",
    "param_grid_rf = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 20, 30]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "    .build()\n",
    "\n",
    "# Set up the cross-validator with the RandomForestClassifier, parameter grid, and desired number of folds\n",
    "cross_validator_rf = CrossValidator(\n",
    "    estimator=rf,\n",
    "    estimatorParamMaps=param_grid_rf,\n",
    "    evaluator=multi_evaluator,  # Use the MulticlassClassificationEvaluator from previous examples\n",
    "    numFolds=4\n",
    ")\n",
    "\n",
    "rf_model = cross_validator_rf.fit(pca_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "rf_predictions = rf_model.transform(pca_test)\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "rf_roc_auc = binary_evaluator.evaluate(rf_predictions, {binary_evaluator.metricName: \"areaUnderROC\"})\n",
    "rf_accuracy = multi_evaluator.evaluate(rf_predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
    "rf_f1_score = multi_evaluator.evaluate(rf_predictions, {multi_evaluator.metricName: \"f1\"})\n",
    "end_time = time.time()\n",
    "print(f'Random Forest evaluation time: {end_time - start_time:.3f} seconds') \n",
    "print(\"Random Forest Classifier:\")\n",
    "print(\"ROC-AUC: {:.6f}\".format(rf_roc_auc))\n",
    "print(\"Accuracy: {:.6f}\".format(rf_accuracy))\n",
    "print(\"F1 Score: {:.6f}\".format(rf_f1_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the elements of the confusion matrix\n",
    "TN = rf_predictions.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = rf_predictions.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = rf_predictions.filter('prediction = 0 AND label <> prediction').count()\n",
    "FP = rf_predictions.filter('prediction = 1 AND label <> prediction').count()\n",
    "# show confusion matrix\n",
    "rf_predictions.groupBy('label', 'prediction').count().show()\n",
    "# calculate metrics by the confusion matrix\n",
    "accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "F =  2 * (precision*recall) / (precision + recall)\n",
    "# calculate auc\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "auc = evaluator.evaluate(rf_predictions, {evaluator.metricName: 'areaUnderROC'})\n",
    "print('n precision: %0.3f' % precision)\n",
    "print('n recall: %0.3f' % recall)\n",
    "print('n accuracy: %0.3f' % accuracy)\n",
    "print('n F1 score: %0.3f' % F)\n",
    "print('AUC: %0.3f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "# Define the layers for the neural network:\n",
    "# Input layer of size 10 (features), two hidden layers of size 20 and 10,\n",
    "# and an output layer of size 2 (classes)\n",
    "\n",
    "layers = [10, 20, 10, 2]\n",
    "start_time = time.time()\n",
    "# Create the trainer and set its parameters\n",
    "mlp = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=42, featuresCol='features', labelCol='label')\n",
    "\n",
    "binary_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(mlp.stepSize, [0.01, 0.05, 0.1]) \\\n",
    "    .addGrid(mlp.solver, ['l-bfgs', 'gd']) \\\n",
    "    .build()\n",
    "\n",
    "# Set up the cross-validator with the Multilayer Perceptron Classifier, parameter grid, and desired number of folds\n",
    "cross_validator = CrossValidator(\n",
    "    estimator=mlp,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=multi_evaluator,  # Use the MulticlassClassificationEvaluator from previous examples\n",
    "    numFolds=4\n",
    ")\n",
    "\n",
    "\n",
    "# Train the neural network model\n",
    "mlp_model = mlp.fit(pca_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "mlp_predictions = mlp_model.transform(pca_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mlp_roc_auc = binary_evaluator.evaluate(mlp_predictions, {binary_evaluator.metricName: \"areaUnderROC\"})\n",
    "mlp_accuracy = multi_evaluator.evaluate(mlp_predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
    "mlp_f1_score = multi_evaluator.evaluate(mlp_predictions, {multi_evaluator.metricName: \"f1\"})\n",
    "end_time = time.time()\n",
    "print(f'Multilayer Perceptron evaluation time: {end_time - start_time:.3f} seconds') \n",
    "print(\"Multilayer Perceptron Classifier:\")\n",
    "print(\"ROC-AUC: {:.6f}\".format(mlp_roc_auc))\n",
    "print(\"Accuracy: {:.6f}\".format(mlp_accuracy))\n",
    "print(\"F1 Score: {:.6f}\".format(mlp_f1_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the elements of the confusion matrix\n",
    "TN = mlp_predictions.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = mlp_predictions.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = mlp_predictions.filter('prediction = 0 AND label <> prediction').count()\n",
    "FP = mlp_predictions.filter('prediction = 1 AND label <> prediction').count()\n",
    "# show confusion matrix\n",
    "mlp_predictions.groupBy('label', 'prediction').count().show()\n",
    "# calculate metrics by the confusion matrix\n",
    "accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "F =  2 * (precision*recall) / (precision + recall)\n",
    "# calculate auc\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "auc = evaluator.evaluate(mlp_predictions, {evaluator.metricName: 'areaUnderROC'})\n",
    "print('n precision: %0.3f' % precision)\n",
    "print('n recall: %0.3f' % recall)\n",
    "print('n accuracy: %0.3f' % accuracy)\n",
    "print('n F1 score: %0.3f' % F)\n",
    "print('AUC: %0.3f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "start_time = time.time()\n",
    "# Create a Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "binary_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "# Create a parameter grid for hyperparameter tuning\n",
    "param_grid_dt = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [5, 10, 15]) \\\n",
    "    .addGrid(dt.maxBins, [16, 32, 64]) \\\n",
    "    .build()\n",
    "\n",
    "# Create a cross-validator\n",
    "cross_validator_dt = CrossValidator(estimator=dt,\n",
    "                                  estimatorParamMaps=param_grid_dt,\n",
    "                                  evaluator=MulticlassClassificationEvaluator(metricName=\"f1\"),\n",
    "                                  numFolds=4)\n",
    "\n",
    "# Train the Decision Tree Classifier using cross-validation\n",
    "cv_model_dt = cross_validator_dt.fit(pca_train)\n",
    "\n",
    "# Get the best Decision Tree Classifier\n",
    "best_dt = cv_model_dt.bestModel\n",
    "\n",
    "# Make predictions on the test set using the best Decision Tree Classifier\n",
    "dt_predictions = best_dt.transform(pca_test)\n",
    "\n",
    "# Evaluate the Decision Tree Classifier\n",
    "binary_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "# roc_auc = binary_evaluator.evaluate(predictions, {binary_evaluator.metricName: \"areaUnderROC\"})\n",
    "# multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "dt_roc_auc = binary_evaluator.evaluate(dt_predictions, {binary_evaluator.metricName: \"areaUnderROC\"})\n",
    "dt_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\").evaluate(dt_predictions)\n",
    "dt_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\").evaluate(dt_predictions)\n",
    "end_time = time.time()\n",
    "print(f'Decision Tree Classifier evaluation time: {end_time - start_time:.3f} seconds') \n",
    "# Print the evaluation metrics\n",
    "print(\"Decision Tree Classifier:\")\n",
    "print(\"ROC-AUC: {:.6f}\".format(dt_roc_auc))\n",
    "print(\"Accuracy: {:.6f}\".format(dt_accuracy))\n",
    "print(\"F1 Score: {:.6f}\".format(dt_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the elements of the confusion matrix\n",
    "TN = dt_predictions.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = dt_predictions.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = dt_predictions.filter('prediction = 0 AND label <> prediction').count()\n",
    "FP = dt_predictions.filter('prediction = 1 AND label <> prediction').count()\n",
    "# show confusion matrix\n",
    "dt_predictions.groupBy('label', 'prediction').count().show()\n",
    "# calculate metrics by the confusion matrix\n",
    "accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "F =  2 * (precision*recall) / (precision + recall)\n",
    "# calculate auc\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "auc = evaluator.evaluate(dt_predictions, {evaluator.metricName: 'areaUnderROC'})\n",
    "print('n precision: %0.3f' % precision)\n",
    "print('n recall: %0.3f' % recall)\n",
    "print('n accuracy: %0.3f' % accuracy)\n",
    "print('n F1 score: %0.3f' % F)\n",
    "print('AUC: %0.3f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "start_time = time.time()\n",
    "# Define the GBT classifier with its parameters\n",
    "gbt = GBTClassifier(maxDepth=5, maxIter=10)\n",
    "\n",
    "binary_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth, [4, 8, 10]) \\\n",
    "    .addGrid(gbt.maxIter, [2, 5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "# Set up the cross-validator with the GBT classifier, parameter grid, and desired number of folds\n",
    "cross_validator = CrossValidator(\n",
    "    estimator=gbt,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"f1\"),\n",
    "    numFolds=4\n",
    ")\n",
    "\n",
    "# Train the GBT model on the training set\n",
    "gbt_model = cross_validator.fit(pca_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "gbt_predictions = gbt_model.transform(pca_test)\n",
    "\n",
    "# Evaluate the GBT model using various evaluation metrics\n",
    "gbt_accuracy = MulticlassClassificationEvaluator(metricName=\"accuracy\").evaluate(gbt_predictions)\n",
    "gbt_f1_score = MulticlassClassificationEvaluator(metricName=\"f1\").evaluate(gbt_predictions)\n",
    "gbt_roc_auc = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\").evaluate(gbt_predictions)\n",
    "\n",
    "# Print the evaluation results\n",
    "end_time = time.time()\n",
    "print(f'GBT Classifier evaluation time: {end_time - start_time:.3f} seconds') \n",
    "print(\"GBT Classifier:\")\n",
    "print(\"Accuracy: {:.6f}\".format(gbt_accuracy))\n",
    "print(\"F1 Score: {:.6f}\".format(gbt_f1_score))\n",
    "print(\"ROC-AUC: {:.6f}\".format(gbt_roc_auc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the elements of the confusion matrix\n",
    "TN = gbt_predictions.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = gbt_predictions.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = gbt_predictions.filter('prediction = 0 AND label <> prediction').count()\n",
    "FP = gbt_predictions.filter('prediction = 1 AND label <> prediction').count()\n",
    "# show confusion matrix\n",
    "gbt_predictions.groupBy('label', 'prediction').count().show()\n",
    "# calculate metrics by the confusion matrix\n",
    "accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "F =  2 * (precision*recall) / (precision + recall)\n",
    "# calculate auc\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "auc = evaluator.evaluate(gbt_predictions, {evaluator.metricName: 'areaUnderROC'})\n",
    "print('n precision: %0.3f' % precision)\n",
    "print('n recall: %0.3f' % recall)\n",
    "print('n accuracy: %0.3f' % accuracy)\n",
    "print('n F1 score: %0.3f' % F)\n",
    "print('AUC: %0.3f' % auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
